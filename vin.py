# -*- coding: utf-8 -*-
"""vin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hPAF-wAlpXvm6Xt-1iYNItD3AK9V2-6-
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, TensorDataset

# ---  Data Generation & Preprocessing ---
class TimeSeriesDataModule:
    """Handles data acquisition and preprocessing for multivariate forecasting."""
    def __init__(self, n_steps=1000, seq_length=24):
        self.n_steps = n_steps
        self.seq_length = seq_length
        self.scaler = MinMaxScaler()

    def generate_synthetic_data(self):
        """Generates a complex multivariate dataset (5 features)."""
        t = np.linspace(0, 100, self.n_steps)
        f1 = np.sin(t) + np.random.normal(0, 0.1, self.n_steps) # Sine wave
        f2 = np.cos(t * 0.5) + np.random.normal(0, 0.1, self.n_steps) # Cosine
        f3 = np.random.normal(0, 1, self.n_steps).cumsum() # Random walk
        f4 = f1 * f2 # Interaction term
        f5 = np.log1p(np.abs(f3)) # Non-linear transformation

        data = np.stack([f1, f2, f3, f4, f5], axis=1)
        return pd.DataFrame(data, columns=['f1', 'f2', 'f3', 'f4', 'target'])

    def prepare_loaders(self, batch_size=32):
        df = self.generate_synthetic_data()
        scaled_data = self.scaler.fit_transform(df)

        X, y = [], []
        for i in range(len(scaled_data) - self.seq_length):
            X.append(scaled_data[i:i+self.seq_length, :])
            y.append(scaled_data[i+self.seq_length, -1]) # Target is the last column

        X, y = torch.FloatTensor(np.array(X)), torch.FloatTensor(np.array(y))

        # Split 80/20
        split = int(0.8 * len(X))
        train_ds = TensorDataset(X[:split], y[:split])
        test_ds = TensorDataset(X[split:], y[split:])

        return DataLoader(train_ds, batch_size=batch_size, shuffle=True), \
               DataLoader(test_ds, batch_size=batch_size), df

# ---  The Custom Attention Layer ---
class Attention(nn.Module):
    """Additive (Bahdanau) Attention Mechanism."""
    def __init__(self, hidden_dim):
        super(Attention, self).__init__()
        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)
        self.v = nn.Linear(hidden_dim, 1, bias=False)

    def forward(self, hidden, encoder_outputs):
        # hidden: [batch, hidden_dim]
        # encoder_outputs: [batch, seq_len, hidden_dim]
        seq_len = encoder_outputs.size(1)
        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)

        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))
        attention_weights = torch.softmax(self.v(energy), dim=1) # [batch, seq_len, 1]

        context = torch.bmm(attention_weights.permute(0, 2, 1), encoder_outputs)
        return context.squeeze(1), attention_weights

# ---  Seq2Seq Model with Attention ---
class AttentionForecaster(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim=1):
        super(AttentionForecaster, self).__init__()
        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.attention = Attention(hidden_dim)
        self.decoder = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # x: [batch, seq_len, input_dim]
        encoder_outputs, (hn, cn) = self.encoder(x)

        # Use the last hidden state to attend over all encoder outputs
        context, weights = self.attention(hn[-1], encoder_outputs)

        prediction = self.decoder(context)
        return prediction, weights

# ---  Training and Evaluation ---
def train_model(model, train_loader, epochs=20):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            output, _ = model(batch_x)
            loss = criterion(output.squeeze(), batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        if (epoch+1) % 5 == 0:
            print(f"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}")

# --- Execution ---
data_mod = TimeSeriesDataModule()
train_loader, test_loader, original_df = data_mod.prepare_loaders()

model = AttentionForecaster(input_dim=5, hidden_dim=64)
train_model(model, train_loader)

model.eval()
with torch.no_grad():
    sample_x, _ = next(iter(test_loader))
    pred, weights = model(sample_x)

    plt.figure(figsize=(10, 4))
    plt.bar(range(24), weights[0].flatten().numpy())
    plt.title("Attention Weights Across Input Sequence")
    plt.xlabel("Time Step (T-24 to T-1)")
    plt.ylabel("Importance Weight")
    plt.show()